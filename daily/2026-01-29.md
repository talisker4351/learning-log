## 2026-01-29

### Today
I learned how gradient boosting works and built my first XGBoost model with basic parameter tuning.

### Insight
XGBoost improves accuracy by iteratively correcting errors, and combining a small learning rate with many trees can yield powerful models.

### Tomorrow
I will experiment with early stopping and learning rate tuning to further optimize model performance.

---

### 今日
勾配ブースティングの仕組みを学び、XGBoostでモデル構築と基本的なパラメータ調整を行った。

### 気づき
誤差を順次補正していくことで精度が高まり、小さな学習率と多くの木の組み合わせが強力だと分かった。

### 明日
early stopping と learning rate の調整を試して、モデル性能をさらに最適化する
