## 2026-01-18.md

**Today**  
I learned how Random Forest models reduce overfitting by averaging predictions from many decision trees.

**Insight**  
Compared to a single decision tree, Random Forest achieved significantly lower MAE, showing stronger generalization on validation data.

**Next**  
Learn imputation strategies that handle missing data without degrading model performance.

---

**今日**  
複数の決定木の予測を平均することで過学習を抑える、ランダムフォレストの仕組みを学んだ。

**気づき**  
単一の決定木よりも MAE が大きく改善され、検証データに対する汎化性能の重要性を再確認した。

**次の学習タイミングで**  
モデル性能を落とさない補完戦略を学ぶ。
